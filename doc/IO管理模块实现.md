# 基于epoll的异步I/O管理框架

又称：基于epoll的异步I/O协程调度

## 一、简介

IO管理器（IOManager）是一个基于epoll的异步I/O管理框架。它负责监听和管理各种文件描述符上的I/O事件，并将这些事件交由调度器处理，主要功能包括：

1. **事件注册与监听**：通过epoll注册文件描述符的读写事件。
2. **事件触发与回调**：在事件发生时，触发相应的回调或协程。
3. **协程调度**：继承自调度器（Scheduler），管理协程的创建、调度和执行。
4. **定时器管理**：支持定时器功能，处理定时任务。
5. **资源管理**：创建和销毁epoll实例、管道，以及文件描述符上下文等资源。

## 二、基础知识

### 2.1  I/O 读写过程

IO (Input/Output，输入/输出)即数据的读取（接收）或写入（发送）操作，通常用户进程中的一个完整IO分为两阶段：<font color='red'>`用户进程空间<-->内核空间`</font>、<font color='red'>`内核空间<-->设备空间（磁盘、网络等）`</font>。IO有**内存IO、网络IO和磁盘IO**三种，通常我们说的IO指的是后两者。

![内核空间和用户空间](F:\学习\研二\学习\cpp项目\figures\内核空间和用户空间.png)

LINUX中进程无法直接操作I/O设备，其必须**通过系统调用请求kernel来协助完成I/O动作**；内核会为每个I/O设备维护一个缓冲区（内核缓冲区）。

以网络I/O为例，其过程如下图所示：

![在这里插入图片描述](F:\学习\研二\学习\cpp项目\figures\网络IO过程.png)

1. 当网络数据包到达网卡时，网卡通过 DMA 的方式将数据放到 RingBuffer （环形缓冲区） 中。
2. 然后向 CPU 发起硬中断，在硬中断响应程序中创建 sk_buffer，并将网络数据拷贝至 sk_buffer 中。
3. 随后发起软中断，内核线程 ksoftirqd 响应软中断，调用 poll 函数将 sk_buffer 送往内核协议栈做层层协议处理（链路层、网络层、传输层）。在传输层 tcp_rcv 函数中，去掉 TCP 头，根据四元组（源IP，源端口，目的IP，目的端口）查找对应的 socket。
4. 最后将 sk_buffer 放到 socket 中的接收队列里。

以上仅为**【过程一】**数据准备的过程（内核程序从网卡/磁盘读取数据到内核空间缓存区）。之后用户进程在发现socket缓冲区中有事件触发（可读）后，进行**【过程二】**：将数据从内核空间拷贝到用户进程缓冲区。

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3e8bd27179c84ced880774224bf77194~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)

### 2.2 文件I/O分类

文件的读写方式各有千秋，对于文件的 I/O 分类也非常多，常见的有：

- 缓冲与非缓冲 I/O
- 直接与非直接 I/O
- 阻塞与非阻塞 I/O VS 同步与异步 I/O

#### ✅缓冲与非缓冲 I/O

 文件操作的C标准库是可以实现数据的缓存，那么根据**「是否利用标准库缓冲」**，可以把文件 I/O 分为缓冲 I/O 和非缓冲 I/O：

- 缓冲 I/O，利用的是**C标准库的缓存实现文件的加速访问**，而标准库再通过系统调用访问文件。
- 非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存。

 这里所说的「缓冲」特指标准库内部实现的缓冲。

 比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，**减少系统调用的次数，毕竟系统调用是有 CPU 上下文切换的开销的**。

#### ✅直接与非直接 I/O

我们都知道磁盘 I/O 是非常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调用后，会把用户数据拷贝到内核中缓存起来，这个内核缓存空间也就是「页缓存」，只有当缓存满足某些条件的时候，才发起磁盘 I/O 的请求。

那么，根据是**「否利用操作系统的缓存」**，可以把文件 I/O 分为直接 I/O 与非直接 I/O：

- 直接 I/O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。
- 非直接 I/O，读操作时，数据从内核缓存中拷贝给用户程序，写操作时，数据从用户程序拷贝给内核缓存，再**由内核决定什么时候写入数据到磁盘**。

**❓ 如果用了非直接 I/O 进行写数据操作，内核什么情况下才会把缓存数据写入到磁盘？**

> 以下几种场景会触发内核缓存的数据写入磁盘：
>
> - 在调用 `write` 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上；
> - 用户主动调用 `sync`，内核缓存会刷到磁盘上；
> - 当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上；
> - 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上

❓ **内核缓冲区和标准库缓冲区是一回事吗？  有什么区别？**

> 不是。
>
> 内核缓冲区是操作系统（Linux内核）中用于存储文件数据的一部分内存。
>
> 标准库缓冲区是C语言标准库（如stdio.h中定义的函数）中提供的一种缓冲机制，是在用户空间的一块内存区域。
>
> 两个的区别可以视为内核缓冲区和用户缓冲区的区别。

**==结论：是否使用内核缓冲区决定了是直接/非直接IO，是否使用标准库缓冲区（用户缓冲区）决定了是缓冲/非缓冲IO。==**

*~阻塞和非阻塞I/O、异步和同步I/O，我们在下一节中详细介绍~*

### 2.3 五种文件I/O模型

针对研究I/O的两个过程：

> 1. 【过程一】数据准备的过程（内核程序从网卡/磁盘读取数据到内核空间缓存区）
> 2. 【过程二】数据从内核空间拷贝到用户进程缓冲区的过程

这两个过程阻塞与否，大体存在五种IO模型包括：**阻塞I/O、非阻塞I/O、信号驱动I/O、I/O多路转接、异步I/O**。

#### (1) 阻塞I/O：【过程一】和【过程二】都会阻塞

**阶段一：**

​	用户进程尝试读取数据，可是数据尚未达到（未准备好）此时内核也是处于等待状态，而用户进程就是阻塞状态。

**阶段二：**

​	此时数据已经到达并来到了内核缓冲区，代表已经就绪；但是数据还只是在内核空间中，并没有被拷贝到用户空间中，所以这时候用户进程还是不能处理数据，	继续阻塞。

​	直到数据拷贝到用户空间中，用户进程才解除阻塞，开始处理数据。

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4ec7043a598b4cb68de0c651a130b70e~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)

#### (2) 非阻塞I/O：【过程一】轮询，【过程二】阻塞

**阶段一：**

​	用户进程尝试读取数据，可是数据尚未达到（未准备好）此时内核是处于等待状态；但是由于是非阻塞IO，此时用户会返回异常，即用户进程并不会阻塞等待；	**用户进程拿到error后，再次尝试读取，循环往复，直到数据就绪**。

​	整个过程和**CPU的轮询**很是相似！

**阶段二：**

​	此时数据已经到达并来到了内核缓冲区，代表已经就绪；但是数据还只是在内核空间中，并没有被拷贝到用户空间中，所以这时候用户进程还是不能处理数据，	继续阻塞。

​	直到数据拷贝到用户空间中，用户进程才解除阻塞，开始处理数据。

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/44c05261e52e4e87a1691f45421221dd~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)

当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么内核它马上就将数据拷贝到了用户内存，然后返回。

所以，**nonblocking IO的特点是需要不断的主动询问kernel数据好了没有**。

问题：在非阻塞IO模型中，**用户线程需要不断地询问内核数据是否就绪**，也就说<font color='red'>**非阻塞IO不会交出CPU，而会一直占用CPU**</font>。无论是阻塞IO和非阻塞IP都不能充分发挥CPU的作用。

#### (3) 多路复用I/O：【过程一】轮询或阻塞等待，【过程二】阻塞

在非阻塞I/O模式下，**应用程序需要不断地轮询各个I/O操作的状态，同时如果是单线程那一次也只能处理一个IO事件**（不能当一个IO事件没准备好时去处理其他的），并没有充分发挥CPU的能力，造成资源浪费。一种直观的方法就是利用多线程机制，为每一个IO事件开辟一个新的线程来处理。**<font color='cornflowerblue'>这样可以达到先去处理就绪的IO事件的目的，而不是只能一个一个来，阻塞在没有就绪的IO事件上。</font>**

然而这样，因为每个轮询都要由一个线程去控制，就会导致存在大量的线程。因此，I/O多路复用就是通过一种机制，**<font color='cornflowerblue'>仅使用一个线程就可以监视多个描述符</font>**，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。这样就实现了单个线程也可以监视多个I/O事件，当哪个事件就绪了，先去处理。

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4ef11446f94549a3be57fd8cbe29178d~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)

常用的多路复用机制包括<font color='red'>`select`</font>, <font color='red'>`poll`</font>, <font color='red'>`epoll`</font>，前两者也是通过轮询文件描述符集合的方式来寻找准备完成的文件描述符，而<font color='red'>`epoll`</font>基于事件驱动机制，不需要对文件描述符集合进行遍历。

#### (4) 信号驱动I/O：【过程二】阻塞

**信号驱动IO**是与内核**<font color='cornflowerblue'>建立SIGIO的信号关联并设置回调，当内核有FD就绪时，会发出SIGIO信号通知用户，期间用户应用可以执行其它业务，无需阻塞等待</font>**。

**阶段一：**

​	①用户进程调用sigaction，注册信号处理函数

​	②内核返回成功，开始监听FD

​	③用户进程不阻塞等待，可以执行其它业务

​	④当内核数据就绪后，回调用户进程的SIGIO处理函数

**阶段二：**

​	①收到SIGIO回调信号

​	②调用recvfrom，读取

​	③内核将数据拷贝到用户空间

​	④用户进程处理数据

![image.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c3a830fe920748618145ed2acfc415fd~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)

**缺点：** 当有大量IO操作时，信号较多，SIGIO处理函数不能及时处理可能导致信号队列溢出，而且内核空间与用户空间的频繁信号交互性能也较低。

> 疑问🤔：epoll和信号驱动的I/O之间的区别？
>
> epoll是事件驱动的多路复用方法，当事件准备就绪时触发返回，然后用户通过系统调用（如`read`函数）将数据从内核读取到用户空间，进而进行后续处理。这个过程与信号驱动的I/O过程十分类似，又有什么区别呢？

- **最主要的区别：epoll在【过程一】本质上是阻塞用户进程的，而信号驱动的I/O不会。**

  信号驱动的I/O在【过程一】中是异步的，当用户进程调用函数后，可以去执行其他业务。等内核数据准备好后，通过<font color='red'>信号的软中断机制</font>使得用户进程回来进行【过程二】。而epoll在注册好事件后，需要<font color='red'>调用epoll_wait函数来等待事件的触发</font>，当事件发生后才会返回来处理。（当然，也可以通过定时器等方案来使得epoll_wait能尽快返回，但这些是额外处理）

- **性能和灵活性：信号驱动的I/O依赖于信号处理，这是十分复杂的，且不适合在多线程机制中使用；而epoll十分灵活，且性能更好**

  信号驱动 I/O 的主要优势在于不阻塞用户进程，可以继续处理其他业务，适合需要异步通知和低延迟的场景。

  `epoll` 默认是阻塞的，但它通过高效的事件通知机制和简单的 API 设计，更适合处理大量并发连接和高性能网络应用。可以通过多线程、非阻塞模式或定时器等方式，实现异步处理逻辑，更加灵活，适合高性能和高并发的应用场景。

#### (5) 异步I/O：不阻塞

**异步IO**的整个过程都是**<font color='red'>非阻塞</font>**的，**用户进程调用完异步API后就可以去做其它事情，内核等待数据就绪并拷贝到用户空间后才会递交信号，通知用户进程**。（这个过程就像是CPU的中断机制一样，用户进程把任务放出去后，就可以去干其他事情了，待内核处理完毕后，内核会给信号用户进程）

> 异步IO和信号驱动IO都是中断模式（信号是软中断），但是两者的区别是，<font color='cornflowerblue'>信号驱动IO在第一阶段结束的时候就发出中断信号了，第二阶段需要用户进程参与</font>；而异步IO则实现了真正的异步，内核只有<font color='cornflowerblue'>把所有东西都处理完了（数据都拷贝回到用户空间了）才会发出中断信号</font>

**阶段一：**

​	①用户进程调用aio_read，创建信号回调函数

​	②内核等待数据就绪

​	③用户进程无需阻塞，可以做任何事情

**阶段二：**

​	①内核数据就绪

​	②内核数据拷贝到用户缓冲区

​	③拷贝完成，内核递交信号触发aio_read中的回调函数

​	④用户进程处理数据

![image.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c4d6b52de1274b18a34182e7f5359ceb~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)

> **总结**

无论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用都是<font color='red'>**同步调用**</font>。因为它们在 `read` 调用时，内核将数据从内核空间拷贝到应用程序空间，**过程都是需要等待的，也就是说这个过程是同步的**，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。

**==五种IO模型，只有最后一种异步IO才是真正的异步（内核空间与用户空间的拷贝过程并不需要用户进程关心），其余均是同步I/O==**

![image.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2e25ef770a32466d800c03fdcfd392b4~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp?)



### 2.4 文件I/O在Linux系统编程中的实现

Linux系统提供了两种主要的文件操作方式：**文件描述符（File Descriptor）**和**标准I/O流（Standard I/O）**。这两种I/O实现方式也对应了前面文件I/O分类中的**<font color='red'>缓冲与非缓冲I/O</font>**。

#### 🅰文件描述符

这种方式又被称为<font color='red'>`文件I/O`</font>、<font color='red'>`不带缓冲的I/O`</font>，就是**操作系统封装了一系列函数接口供应用程序使用，通过这些接口可以实现对文件的读写操作**。

文件描述符是一个非负整数，用于唯一标识一个打开的文件。通常情况下：

- 标准输入（stdin）、标准输出（stdout）、标准错误（stderr）分别使用文件描述符 0、1、2。
- 其他打开的文件会有不同的文件描述符。

文件I/O是**采用系统直接调用**的方式，因此当使用这些接口对文件进行操作时，就会立刻触发系统调用过程，即向系统内核发出请求之后，系统内核会收到执行相关代码处理的请求，决定是否将操作硬件资源或返回结果给应用程序。常见的文件I/O函数包括：

- **open()**：用于打开或创建一个文件。
- **read()**：从文件描述符中读取数据。
- **write()**：向文件描述符中写入数据。
- **close()**：关闭一个文件描述符。

这些函数直接操作文件描述符，是系统调用（System Call）的一部分，提供了对文件的底层访问能力。

#### 🅱标准I/O流

又称<font color='red'>`带缓冲的I/O`</font>，来自C标准库，提供了一组高级别的文件I/O函数，它们建立在文件描述符的基础上，简化了文件操作。主要包括：

- **FILE结构体**：用于表示文件流的结构体，封装了文件描述符以及相关的缓冲区等信息。
- **fopen()**：用于打开一个文件，并返回一个FILE结构体指针。
- **fclose()**：关闭一个文件流。
- **fread()**、**fwrite()**：用于读取和写入文件流。
- **fprintf()**、**fscanf()**：类似于printf()和scanf()，但操作文件流。

标准I/O函数提供了**<font color='cornflowerblue'>缓冲、格式化输入输出</font>**等功能，更易于使用和理解，但在底层实现上仍然依赖于文件描述符。

## 三、I/O管理框架（I/O协程调度）

IO协程调度可以看成是增强版的协程调度。

在前面的协程调度模块中，调度器对协程的调度是无条件执行的，在调度器已经启动调度的情况下，任务一旦添加成功，就会排队等待调度器执行。调度器不支持删除调度任务，并且调度器在正常退出之前一定会执行完全部的调度任务，所以在某种程度上可以认为，把一个协程添加到调度器的任务队列，就相当于调用了协程的resume方法。

IO协程调度支持协程调度的全部功能，因为IO协程调度器是直接继承协程调度器实现的。除了协程调度，**<font color='red'>IO协程调度还增加了IO事件调度的功能，这个功能是针对描述符（一般是套接字描述符）的</font>**。IO协程调度支持**为描述符注册可读和可写事件的回调函数（协程）**，当描述符可读或可写时，执行对应的回调函数。（这里可以直接把回调函数等效成协程，所以这个功能被称为IO协程调度）

> IO事件调度功能对服务器开发至关重要，因为服务器通常需要处理大量来自客户端的socket fd，使用IO事件调度可以将开发者从判断socket fd是否可读或可写的工作中解放出来，使得程序员只需要关心socket fd的IO操作。后续的socket api hook模块也依赖IO协程调度。
>
> 很多的库都可以实现类似的工作，比如libevent，libuv，libev等，这些库被称为异步事件库或异步IO库，从网上可以很容易地找到大把的资料介绍这类库。有的库不仅可以处理socket fd事件，还可以处理定时器事件和信号事件。
>
> 这些事件库的实现原理基本类似，都是先将套接字设置成非阻塞状态，然后将套接字与回调函数绑定，接下来进入一个基于IO多路复用的事件循环，等待事件发生，然后调用对应的回调函数。这里可以参考一个基于epoll实现的简单事件库：[3.2 epoll的反应堆模式实现 · libevent深入浅出 · 看云](https://www.kancloud.cn/aceld/libevent_aceld/1858559)，sylar的IO调度和这种写法类似。

### 3.1 模块拆解

所涉及到的模块可以封装为以下几个类：

- **IOManager**：管理所有 I/O 事件，包括添加、删除、取消和触发事件。
- **Scheduler**：调度任务，管理协程和回调函数。
- **FdContext**：封装文件描述符的上下文信息，包括读写事件和对应的回调或协程。
- **EventContext**：封装具体事件的上下文信息，包括调度器、协程和回调函数。

它们之间的关系

> - `IOManager` 继承自 `Scheduler` 和`TimerManager`，用于调度任务和管理协程及回调函数，可以同时管理任务和定时器
> - `IOManager` 包含多个 `FdContext` 实例，存在数组中，用于封装每个文件描述符的上下文信息
> - `FdContext` 包含两个 `EventContext` 实例，分别用于处理读事件和写事件
> - `EventContext` 与 `Scheduler`关联，用于执行具体的事件回调或协程
> - `EventContext` 关联一个 `Fiber`，用于协程的管理和执行

![iomanager](F:\学习\研二\学习\cpp项目\figures\iomanager.png)



所以说，**I/O协程调度模块本质上还是协程调度模块（继承），同时加入了定时器**，且增加了I/O操作，而不是普通协程任务。引入定时器的主要目的是处理以下几个方面的需求：

- **超时处理**：定时器用于处理超时事件，即当等待IO事件的时候，设置一个超时时间，如果超过这个时间还没有事件发生，就需要执行超时处理逻辑。这种机制可以防止因为某些IO操作长时间未完成而导致系统资源的浪费或者卡死。
- **性能优化**：通过定时器设置合理的超时时间，可以在一定程度上优化系统的响应速度和吞吐量。合理的超时设置可以保证即使没有IO事件发生，也能及时返回执行其他任务，提高系统整体的并发处理能力。
- **资源管理**：定时器还可以用于管理系统的资源，例如定期清理或者释放长时间处于空闲状态的资源，从而提高系统的资源利用率。

对于IO协程调度来说，每次调度都包含一个三元组信息，分别是描述符-事件类型（可读或可写）-回调函数，调度器记录全部需要调度的三元组信息，其中描述符和事件类型用于epoll_wait，回调函数用于协程调度。这个三元组信息在源码上通过`FdContext`结构体来存储，在执行epoll_wait时通过epoll_event的私有数据指针data.ptr来保存FdContext结构体信息。

IO协程调度器**在idle时会epoll_wait所有注册的fd，如果有fd满足条件，epoll_wait返回，从私有数据中拿到fd的上下文信息，并且执行其中的回调函数**。（实际是idle协程只负责收集所有已触发的fd的回调函数并将其加入调度器的任务队列，真正的执行时机是idle协程退出后，调度器在下一轮调度时执行）。**此时，这里的处理逻辑就不是忙等待了，和Scheduler中有区别。**

与协程调度器不一样的是，IO协程调度器**支持取消事件**。取消事件表示不关心某个fd的某个事件了，如果某个fd的可读或可写事件都被取消了，那这个fd会从调度器的epoll_wait中删除。

### 3.2 





## 参考资料

[Linux的5种网络IO模型详解 - 苦涩的茶 - 博客园 (cnblogs.com)](https://www.cnblogs.com/liushui-sky/p/12917347.html)

[【linux内核】五大经典IO模型（原理+动图+代码详解） - 掘金 (juejin.cn)](https://juejin.cn/post/7129070726249709599)

[IO协程调度模块 - 类库与框架 - 程序员的自我修养 (midlane.top)](https://www.midlane.top/wiki/pages/viewpage.action?pageId=10061031)